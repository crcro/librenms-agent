#!/usr/bin/env python3
import warnings
import re
import MySQLdb
import base64
import json
import os.path
warnings.filterwarnings(action="ignore", message='the sets module is deprecated')

temp_file = '/tmp/mysql-stats.old'


conn = MySQLdb.connect(host='', user='', passwd='', db='mysql')

cursor = conn.cursor()

cursor.execute("SHOW GLOBAL STATUS")
rows = cursor.fetchall()

datavariables = {
        'Command Counters': ['Com_delete','Com_insert','Com_insert_select','Com_load','Com_replace','Com_replace_select', 'Com_select', 'Com_update', 'Com_update_multi'],
        'Connections': ['max_connections', 'Max_used_connections', 'Aborted_clients', 'Aborted_connects','Threads_connected','Connections'],
        'Files and Tables': ['table_open_cache','Open_files','Open_tables','Opened_tables'],
        'InnoDB Buffer Pool': ['ib_bpool_size','ib_bpool_dbpages', 'ib_bpool_free','ib_bpool_modpages'],
        'InnoDB Buffer Pool Activity': ['ib_bpool_read','ib_bpool_created', 'ib_bpool_written'],
        'InnoDB Insert Buffer': ['ib_merged_ops_insert', 'ib_merged_ops_delete', 'ib_discarded_ops_insert', 'ib_discarded_ops_delete', 'ib_search_cache', 'ib_search_non_cache'],
        'InnoDB IO': ['ib_io_read','ib_io_write','ib_io_log', 'ib_io_fsync'],
        'InnoDB IO Pending': ['ib_iop_log','ib_iop_sync', 'ib_iop_flush_log', 'ib_iop_flush_bpool', 'ib_iop_ibuf_aio','ib_iop_aioread','ib_iop_aiowrite'],
        'InnoDB Log': ['innodb_log_buffer_size','ib_log_flush','ib_log_written'],
        'InnoDB Row Operations': ['Innodb_rows_deleted','Innodb_rows_inserted','Innodb_rows_read','Innodb_rows_updated'],
        'InnoDB Semaphores': ['ib_spin_rounds','ib_spin_waits','ib_os_waits'],
        'InnoDB Transactions': ['ib_tnx'],
        'MyISAM Indexes': ['Key_read_requests','Key_reads','Key_write_requests','Key_writes'],
        'Network Traffic': ['Bytes_received','Bytes_sent'],
        'Query Cache': ['Qcache_queries_in_cache','Qcache_hits','Qcache_inserts','Qcache_not_cached','Qcache_lowmem_prunes'],
        'Query Cache Memory': ['query_cache_size','Qcache_free_memory'],
        'Select Types': ['Select_full_join','Select_full_range_join','Select_range','Select_range_check','Select_scan'],
        'Slow Queries': ['Slow_queries'],
        'Sorts': ['Sort_rows','Sort_range','Sort_merge_passes','Sort_scan'],
        'Table Locks': ['Table_locks_immediate','Table_locks_waited'],
        'Temporary Objects': ['Created_tmp_disk_tables','Created_tmp_tables','Created_tmp_files']
        }

data = {}
for row in rows:
    data[row[0]] = row[1]

cursor = ""
cursor = conn.cursor()
cursor.execute("SHOW VARIABLES")
rows = cursor.fetchall()

for row in rows:
    data[row[0]] = row[1]

cursor = ""
cursor = conn.cursor()
cursor.execute("SHOW ENGINE INNODB STATUS")
rows = cursor.fetchall()


for row in rows:
    ib_insert_buffer_merged_ops = re.search(r"merged\soperations\:.n\sinsert\s(\d+).\sdelete\smark\s(\d+).\sdelete\s(\d+)", str(row))
    ib_insert_buffer_discarded_ops = re.search(r"discarded\soperations\:.n\sinsert\s(\d+).\sdelete\smark\s(\d+).\sdelete\s(\d+)", str(row))
    ib_insert_buffer_search_ops = re.search(r"(\d+.\d+)\shash\ssearches/s.\s(\d+.\d+)\snon-hash\ssearches/s", str(row))

    if str(ib_insert_buffer_merged_ops.group(0)):
        data['ib_merged_ops_insert'] = ib_insert_buffer_merged_ops.group(1)
        data['ib_merged_ops_delete'] = ib_insert_buffer_merged_ops.group(3)

    if str(ib_insert_buffer_discarded_ops.group(0)):
        data['ib_discarded_ops_insert'] = ib_insert_buffer_discarded_ops.group(1)
        data['ib_discarded_ops_delete'] = ib_insert_buffer_discarded_ops.group(3)

    if str(ib_insert_buffer_search_ops.group(0)):
        data['ib_search_cache'] = ib_insert_buffer_search_ops.group(1)
        data['ib_search_non_cache'] = ib_insert_buffer_search_ops.group(2)

    for line in row[2].split("\n"):
        ib_bpool_size = re.match(r"Buffer\spool\ssize\s+(\d+)", line)
        ib_bpool_free = re.match(r"Free\sbuffers\s+(\d+)", line)
        ib_bpool_dbpages = re.match(r"Database\spages\s+(\d+)", line)
        ib_bpool_modpages = re.match(r"Modified\sdb\spages\s+(\d+)", line)
        ib_b_reg = re.match(r"Pages\sread\s(\d+),\screated\s(\d+),\swritten (\d+)", line)
        ib_io = re.match(r"(\d+)\sOS\sfile\sreads,\s(\d+)\sOS\sfile\swrites,\s(\d+)\sOS\sfsyncs", line)
        ib_io_log = re.match(r"(\d+)\slog\si\/o's\sdone.*", line)
        ib_io_p1 = re.match(r"Pending\snormal\saio\sreads:\s(\d+).+,\saio\swrites:\s(\d+).+,", line)
        ib_io_p2 = re.match(r"\s?ibuf\saio\sreads:\s(\d+),\slog\si\/o's:\s(\d+),\ssync\si\/o's:\s(\d+)", line)
        ib_io_p3 = re.match(r"\s?Pending\sflushes\s\(fsync\)\slog:\s(\d+);\sbuffer\spool:\s(\d+)\s?", line)
        ib_log_p1 = re.match(r"\s?Log\ssequence\snumber\s([[a-fA-F\d]+)(?: (\d+))?", line)
        ib_log_p2 = re.match(r"\s?Log\sflushed\sup\sto\s+([[a-fA-F\d]+)(?: (\d+))?", line)
        ib_semaphore = re.match(r"\s?Mutex\sspin\swaits\s(\d+),\srounds\s(\d+),\sOS waits\s(\d+)", line)
        ib_tnx = re.match(r"\s?Trx\sid\scounter\s([[a-fA-F\d]+)(?: (\d+))?", line)

        if ib_bpool_size:
            data['ib_bpool_size'] = ib_bpool_size.group(1)
        elif ib_bpool_free:
            data['ib_bpool_free'] = ib_bpool_free.group(1)
        elif ib_bpool_dbpages:
            data['ib_bpool_dbpages'] = ib_bpool_dbpages.group(1)
        elif ib_bpool_modpages:
            data['ib_bpool_modpages'] = ib_bpool_modpages.group(1)
        elif ib_io:
            data['ib_io_read'] = ib_io.group(1)
            data['ib_io_write'] = ib_io.group(2)
            data['ib_io_fsync'] = ib_io.group(3)
        elif ib_io_log:
            data['ib_io_log'] = ib_io_log.group(1)
        elif ib_io_p1:
            data['ib_iop_aioread'] = ib_io_p1.group(1)
            data['ib_iop_aiowrite'] = ib_io_p1.group(2)
        elif ib_io_p2:
            data['ib_iop_ibuf_aio'] = ib_io_p2.group(1)
            data['ib_iop_log'] = ib_io_p2.group(2)
            data['ib_iop_sync'] = ib_io_p2.group(3)
        elif ib_io_p3:
            data['ib_iop_flush_log'] = ib_io_p3.group(1)
            data['ib_iop_flush_bpool'] = ib_io_p3.group(2)
        elif ib_log_p1:
            data['ib_log_written'] = ib_log_p1.group(1)
            if ib_log_p1.group(2):
                data['ib_log_written'] = int(data['ib_log_written']) + int(ib_log_p1.group(2))
        elif ib_log_p2:
            data['ib_log_flush'] = ib_log_p2.group(1)
            if ib_log_p2.group(2):
                data['ib_log_flush'] = int(data['ib_log_flush']) + int(ib_log_p2.group(2))
        elif ib_semaphore:
            data['ib_spin_waits'] = ib_semaphore.group(1)
            data['ib_spin_rounds'] = ib_semaphore.group(2)
            data['ib_os_waits'] = ib_semaphore.group(3)
        elif ib_tnx:
            data['ib_tnx'] = ib_tnx.group(1)
            if ib_tnx.group(2):
                data['ib_tnx'] = int(data['ib_tnx']) + int(ib_tnx.group(2))
        elif ib_b_reg:
            data['ib_bpool_read'] = ib_b_reg.group(1)
            data['ib_bpool_created'] = ib_b_reg.group(2)
            data['ib_bpool_written'] = ib_b_reg.group(3)

points_data = []
points_label = []
for category in datavariables:
    for variable in datavariables[category]:
        if variable in data:
            points_label.append(variable.lower())
            points_data.append(int(float(data[variable])))

entries = dict(zip(points_label, points_data))

if os.path.isfile(temp_file):
    with open(temp_file, 'r') as file:
        old_data = json.loads(file.read().replace("\'","\""))

    new_points_data = []
    new_points_label = []

    for point in old_data:
        new_points_label.append(point)
        new_points_data.append(int(entries[point]) - int(old_data[point]))

    new_entries = dict(zip(new_points_label, new_points_data))
else:
    new_entries = json.loads(str(entries).replace("\'","\""))

with open(temp_file, 'w') as file:
    file.write(str(entries))

export = {"data": new_entries, "error": "0", "errorString": "", "version": "1"}
data = re.sub(' ', '', json.dumps(export))
print(data)